\section{RationalGRL Evaluation Experiment}
\label{sect:validation}

In addition to the coding analysis (Section~\ref{sect:gmas}), which forms the basis of the RationalGRL framework, we also performed a user evaluation with 16 expert users. The objective of this evaluation was to determine whether the users found the components of RationalGRL (arguments, argumentation semantics, critical questions) useful, and whether they found it easier to keep track of and express opinions and beliefs using RationalGRL compared to standard GRL. Regarding the usefulness of the RationalGRl features and the to GRL, the following two broad hypotheses follow from our main points in this paper:

\begin{itemize}
\item[H1] The components of RationalGRL -- arguments, critical questions and determination of the status of arguments -- are a useful extension to standard goal modeling in GRL.
\item[H2] The components of RationalGRL make it easier to express, determine the effect of and communicate to other stakeholders one's opinions and beliefs about a goal model.
\end{itemize}

In the rest of this section, we will describe our experiment in more detail and further specify these hypotheses.

In addition, we wanted to know some basics about the user experience. The aim here is not to conduct a full user experience study for the RationalGRL tool. Rather, we are interested to know if the user experience of the tool is either very good (easy to use) or very bad (difficult to use), as this could influence the participants ideas about the RationalGRL framework and language -- a very nice and easy to use tool tends to make people more positive towards a particular modeling language, whereas irritations about a bad tool will lead to a more negative disposition. 

\subsection{Experiment design}

The idea was to have our participants perform a small modeling task, and then ask them what they thought of RationalGRL. Thus, our experiment consists of three parts:

\begin{enumerate}
\item Explanation: We explain to the participants the basics of the RationalGRL development process and the RationalGRL tool.
\item Modeling: We ask the participants to model a summarized discussion in the RationalGRL tool.
\item Survey: We present the participants with a survey containing questions about the features of RationalGRL, and some questions about the usability of the RationalGRL tool.
\end{enumerate}

The instructions for these three parts are provided in a single document that we sent out to the participants.\footnote{See \url{www.rationalgrl.com}, page ``Empirical Study''} The explanation (Part 1) starts with a general explanation of GRL and standard goal modeling similar to the explanation provided in Section \ref{sect:background:grl}, then briefly discusses the components of RationalGRL (argument, attack, critical questions) and finally the tool is briefly described (similar to Section \ref{sect:tool}). 

For Part 2, we provided a small section of a transcript and some context information (see Appendix~\ref{sect:survey}) and asked the participants to model this in the RationalGRL tool and take a screenshot of their final model. The idea is that the participants act as if they are ``present'' at a discussion about the early-stage requirements and are asked to model the goal model corresponding to these requirements. 

In order to allow the participants to compare their models to standard GRL models, after the modeling exercise we provide the participants with Figure~\ref{fig:example-small}, which models the discussion for Part 2 (Appendix~\ref{sect:survey}) in standard GRL.

For Part 3, we provided the participants with a survey.\footnote{https://goo.gl/forms/fDSMUnAV20wy7kbY2} The survey starts with general questions about the participants (years of experience etc.). The second part of the survey concerns the specific features of RationalGRL, asking participants to rate RationalGRL on a Likert scale from 1 (very useless) to 5 (very useful):
\begin{itemize}
\item[Q1] Do you think the arguments and counterarguments of RationalGRL are a useful extension to standard goal modeling?
\item[Q2] Do you think the critical questions and answers in the details pane are a useful extension to standard goal modeling?
\item[Q3] Do you think the automatic determination of the status of arguments and elements is a useful extension to standard goal modeling?
\end{itemize}
The following questions ask the participants to rate RationalGRL vs. a standard goal modeling language on a Likert scale from 1 (much more difficult) to 5 (much easier):
\begin{itemize}
\item[Q4] Do you think using RationalGRL instead of a standard goal modeling language makes it easier or more difficult to for someone to express beliefs and opinions in a goal model?
\item[Q5] Do you think using RationalGRL instead of a standard goal modeling language makes it easier or more difficult to for someone to determine the effect of beliefs and opinions on the resulting goal model?
\item [Q6] Do you think using RationalGRL instead of a standard goal modeling language makes it easier or more difficult to for someone who is not the original author to understand the goal model? 
\end{itemize}
We also ask two open questions regarding the strengths and weaknesses of RationalGRL compared to GRL. The survey further contains questions about the RationalGRL tool user experience, such as whether it was easy or difficult to get started with the tool and to model the example case. We, furthermore, ask open questions about the strengths and weaknesses of the tool, possible improvements and whether the participant thinks the tool could be used in practice in future. 

In order to test our main hypotheses H1 and H2, we have to formulate the appropriate null hypotheses and alternative hypotheses. For H1 and H2, we hypothesize that the participants will, on average, rate the components of RationalGRL as useful (rating 4) or very useful (rating 5) for Q1, Q2 and Q3, and as making it easier (rating 4) or significantly easier (rating 5) for questions Q4, Q5 and Q6. In other words, the null hypothesis and alternative hypothesis for H1 and H2 are as follows:
\begin{itemize}
\item H1$_{0}$ and H2$_{0}$: median rating for questions Q1-Q6 is 3 or lower.
\item H1$_{a}$ and H2$_{a}$: median rating for questions Q1-Q6 is higher than 3.
\end{itemize}
Note that for each question (Q1-Q6), we can test the null and alternative hypothesis separately, giving insight into exactly which components of RationalGRL were deemed useful. We set the significance level for each hypothesis at $\alpha = 0.05$.

\paragraph{Participants}
We asked 16 participants in our network to participate in the experiment. Most of the participants were therefore either employed in industry or staff and (ex-)students from university. For those working in industry, all participants took the experiment outside of working hours to avoid conflicts of interest. Two thirds of the participants had either a PhD or Master degree, 20\% a Bachelor degree, and one respondent responded with ``Other''. All but one participant had a year or more experience with software development. The average experience was 6.2 years (standard deviation 6.7), with 10 participants having less than 5 years of experience, 6 participants having 8 or more years of experience and one participant having 25 years of experience. The participants also judged themselves to be quite competent in  early-phase requirements engineering: on average they gave themselves a rating of 3.2 out of 5 (standard deviation 1.3) with half of the participants claiming they were at least ``competent'' (rating 4) or ``very competent'' (rating 5). However, experience with goal modeling languages was markedly less: the average rating was 1.9 (standard deviation 1.3), with 9 participants claiming never used a goal modeling language (rating 1), and only two participants displaying regular use (monthly, rating 4, or weekly, rating 5). The participants that had experience with goal modeling languages had mostly used i* (5 users), with 2 users being familiar with GRL and 2 users having used another goal modeling language.

Note that the third author of the paper did not participate in recruiting of the participants, data collection and data processing of the user study due to the limitation of ``Institutional Review Board for the Protection of Human Subjects (IRB)" at her home university. Thus, she has not seen the participants information, the raw and processed data.

\subsection{Results}\label{sec:survey:results}

\begin{table*}[t]
\centering
\begin{tabularx}{0.95\textwidth}{l|l|l|l|l|l||l|l|l|l|l}
& very useless & useless & neutral & useful & very useful & median & mode & p-value \\
\hline
Q1 & 0 (0.0\%) & 0 (0.0\%)  & 3 (21.4\%) & 8 (57.1\%) & 3 (21.4\%) & 4 & 4 & 0.0005 \\
Q2 & 0 (0.0\%) & 3 (21.4\%) & 3 (21.4\%) & 6 (42.9\%) & 2 (14.3\%) & 4 & 4 & 0.1133 \\
Q3 & 0 (0.0\%) & 0 (0.0\%)  & 2 (14.3\%) & 7 (50.0\%) & 5 (35.7\%) & 4 & 4 & 0.0002 
\end{tabularx}
\caption{Participant ratings and statistical results of the usefulness of the components of RationalGRL}
\label{table:survey:table2}
\end{table*}

\begin{table*}[t]
\centering
\begin{tabularx}{0.95\textwidth}{l|l|l|l|l|l||l|l|l|l|l}
& very difficult & difficult & neutral & easy & very easy & median & mode & p-value \\
\hline
Q4  & 0 (0.0\%) & 0 (0.0\%) & 4 (30.8\%) & 4 (30.8\%) & 5 (38.5\%) & 4 & 5 & 0.0020 \\
Q5  & 0 (0.0\%) & 0 (0.0\%) & 6 (46.2\%) & 5 (38.5\%) & 2 (15.4\%) & 4 & 3 & 0.0078\\
Q6 & 0 (0.0\%) & 1 (8.3\%) & 4 (33.3\%) & 7 (58.3\%) & 0 (0.0\%)  & 4 & 4 & 0.0352
\end{tabularx}
\caption{Participant ratings and statistical results of whether the components of RationalGRL make reasoning about a goal model easier}
\label{table:survey:table3}
\end{table*}

First, the participants had to perform the modeling task. They produced RationalGRL models containing on average three arguments (in addition to goals, tasks, etc.) from the short transcript. Some of the examples of RationalGRL models, created by the users, can be found in Appendix~\ref{sect:survey-screenshots}.\footnote{The full results of the survey can be downloaded from \url{www.rationalgrl.com}, on the page ``Empirical study''.}

After the modeling task, the survey questions were asked. We first asked the participants some questions about user experience. The users where generally positive about RationalGRL's clear User Interface (UI), the fact that the tool gives a nice overview of the goals and opinions in a case and that it was straightforward to use and understand. With respect to weaknesses and improvements to the RationalGRL tool, the users mentioned additional UI functionalities such as the possibility to save models, having an ``undo'' function, and flipping the arrow after adding them. Users also suggested various language-related improvements. Some users mentioned they missed the possibility to attack links, while others mentioned that not all GRL elements are supported (for instance, it is currently not possible to add actors). 

The next set of questions concerned the specific features of the RationalGRL modeling language and the difference with standard goal modeling languages. Table~\ref{table:survey:table2} shows the respondents' answers to questions Q1-Q3, and Table~\ref{table:survey:table3} shows the answers to questions Q4-Q6. We concluded with two open questions about the comparison between RationalGRL and other goal modeling languages. When asked about the advantages of RationalGRL over standard goal modeling languages, many users agreed that making arguments explicit may force end users to have a more structured discussion:  ``Yes, having the possibility to add arguments seems quite useful'', `Generally I think its useful to explicitly document arguments of a discussion'', ``Clear communication about argumentation and forcing people to think in those clear terms.'', ``...you can add arguments and that you can answer questions that help you to develop arguments'', ``It's useful that discussion and explanation are close to the diagrams'', ``a way to see how decisions are being shaped.''. Furthermore, one user stated that RationalGRL successfully ``tries to capture the rationale behind the modeling process' and ``has a simple way to compute the status of the arguments''. 

When asked about the weaknesses, users mentioned the complexity as the most important weakness: ``The apparent increase in complexity might lead to negative perceptions'', ``adding yet another layer of complexity scares me''. The concern that the modeling process may involve too much cognitive overhead was mentioned a few times though: ``I think the overhead of inputting a (detailed) discussion in a structured manner into any system makes adoption difficult'', ``the manual input is too complex and takes too much time. An automated process of parsing the conversation log would be much more helpful'', and ``I still believe in the value of arguments, but there should be less confusing ways to capture them''. One user mentioned that complexity and cognitive overload is a problem with goal modeling in general: ``Goal models are already complex (...) I have worked for years on the effect of context on goal models, and my conclusion is that this was very interesting academic work but with close-to-zero practical implications, unfortunately.''.

\subsection{Analysis of Results}\label{sect:validation:analysis}

\paragraph{Hypothesis H1}
The first analysis concerns hypothesis H1, whether the components of RationalGRL -- i.e. arguments, critical questions and determination of the status of arguments -- are a useful extension to standard goal modelling in GRL. Table~\ref{table:survey:table2} summarizes the medians, modes, and p-values for a one-tailed single-sample sign test for the relevant questions Q1-Q3. We chose the simplest non-parametric test available so as to have to take as few assumptions as possible w.r.t. the distribution of answers for each question. As it is evident from the results, the participants were generally positive about the components of RationalGRL. For Q1 and Q3 the p-values are both significantly lower than $\alpha = 0.05$, which means that for these questions we can accept H1$_{a}$ and reject H1$_{0}$. This means that the addition of arguments and the use of arguments to determine the acceptability of elements of the goal model are deemed to be useful to very useful by the participants. As can be seen in Table~\ref{table:survey:table2}: no participant found the addition of arguments and argument status useless or very useless, and 78.6\% (Q1) respectively 85.7\% (Q3) found the new components useful or very useful. Furthermore, these findings fit with the answers to the open questions (see Section \ref{sec:survey:results}). For Q2, the p-value indicates that the result is not significant with $\alpha = 0.05$, which means we cannot reject the null hypothesis H1$_{0}$. This means that the participants did not think the addition of critical questions like to the RationalGRL tool was significantly useful. Thus, we can say that the addition of critical questions is perhaps not as useful as we had hypothesized. This again fits the descriptive statistics in Table\ref{table:survey:table2}: 21.4\% of respondents found critical questions ``useless''. Furthermore, in the open questions a number of participants indicated that the exact difference between some of the critical questions was unclear, and that some of the critical questions seemed unnecessary, at least for the small modeling exercise given in the experiment.

\paragraph{Hypothesis H2}
The second hypothesis states that components of RationalGRL make it easier to express one's opinions and beliefs about a goal model, determine the effect of one's opinions and beliefs about a goal model and communicate one's opinions and beliefs about a goal model to other stakeholders. Table~\ref{table:survey:table3} provides a summary of the statistical analysis of the relevant questions Q4-Q6. As can be seen, all the scores are significantly higher than the median of 3 for $\alpha = 0.05$, which means that we can accept H2$_{a}$ and reject H2$_{0}$ for all the separate questions Q4-Q6. In other words, the fact that the participants rate the components of RationalGRL as making it, on average, easier to reason with their beliefs and opinions about a goal model is very unlikely to be due to chance. 

With respect to Q4, participants stated that arguments make it much easier to express their opinions. None of the participants found expressing their beliefs (much) more difficult, and 69.3\% found it easier or much easier. This corresponds to the answers to Q1, where participants said the arguments were a useful addition. For Q5, participants agree that RationalGRL makes it easier to determine the effect of one's beliefs on the goal model. Combining this with the results for question Q3, which asked about the usefulness of determining the status, we can say that the participants overall found this a useful feature which makes working with conflicting beliefs easier. One participant remarked that ``The reasoning [in the example] seems simplistic [...] the added value of [the formal argumentation] would be justified if the reasoning is more complex''. Thus, the simplistic case in the modeling exercise possibly detracts from the results for Q5 and Q3. Finally, participants were less sure that arguments would help in communicating the goal model to others. This may be due to the fact that in the experiment, communication of opinions to others was not really tested, leaving the participants to guess what this effect would be.  

\subsection{Threats to validity}
Internal validity is about the validity of the experiment results given our experiment setup and interpretation of findings. The main threat in this regard is that the modeling task the participants were asked to do is not realistic. The task is quite small, and based on an existing transcript of a requirements discussion. In a realistic setting, users of RationalGRL would model much more complex requirements together with the stakeholders. The problems with complexity and cognitive overload only start to play a role for larger models, so while the participants were generally positive in the ranking questions Q1-Q6, some reservations were expressed in the open questions as to whether this approach would work in a realistic setting. 

Another threat to internal validity is that many of the participants had no experience with goal modeling languages at all. Hence, the perceived usefulness of the arguments could be influenced by the fact that the participants, for the first time, were given a goal modeling language to perform structured requirements analysis. In other words, perhaps they found a structured modeling language generally quite useful. One way to better check the difference between standard goal modeling and RationalGRL would be to first provide users with a standard goal modeling tool and then have them use RationalGRL. An even more thorough way to test the usefulness of RationalGRL over, for example, standard GRL would be to compare a test and control group, where the test group uses RationalGRL and the control group uses standard GRL. We leave these types of further empirical studies for future research.

External validity concerns the generalizability of the results. As the number of participants is fairly small (16), this threatens the generalizability of our findings. However, the type of participants is realistic: software engineers having a lot of experience with requirements engineering, as opposed to, for example, engineering students with little experience. Furthermore, the results are in line with existing research on design rationale. On the one hand, these existing studies, like our study, show that structured modeling languages suffer from the common problem of high cognitive overhead \cite{shum2006hypermedia}. One the other hand, these studies show that the inclusion of argumentation and critical questions in design and requirements discussions improve the reasoning \cite{razavian2016two,TangEtal2018}.

\subsection{Discussion}
Although our empirical evaluation is relatively small-scale, the results point in some interesting directions for further discussion and research. 

\begin{enumerate}
\item \emph{Arguments and automatically determining acceptability are useful.} Participants were enthusiastic about adding arguments to a goal modeling language with a clear formal underpinning, and they believed the argumentation semantics we use in the tool are intuitive. This is a positive signal for the addition of arguments to goal modeling languages, and also for our formal approach, as just adding arguments as, for example, labels on goals and tasks does not allow one to compute the status of elements in a goal model in the way our approach does. Furthermore, automatically determining the status of elements will have a bigger impact when working with larger, more complex goal models, for which manually determining the impact of arguments is not feasible.
\item \emph{Critical questions need to be clear and have a prominent place in the process of building a goal model.} While participants overall found the critical questions (CQ's) useful, their impact was markedly less than the addition of arguments. This can have various causes. Participants mentioned they did not use the details pane of the tool, which can be used to ask and answer critical questions, that much. Furthermore, it was also mentioned that the difference between different questions was not always clear. Finally, the experiment task, modeling an existing discussion, may also have influenced the CQ's perceived usefulness. The CQ's are meant to be asked during the actual RationalGRL development process in which goal models are constructed and critically analyzed from scratch. 
\item \emph{High cognitive overhead.} A concern that was raised often in our empirical evaluation is that, in its current form, RationalGRL has a relatively high cognitive overhead. Goal modeling is by itself already a cognitively high-effort activity, and the fact that we add more elements to the language does not improve this. 
\end{enumerate}

Taking the above into account, it seems natural in future work to focus more on the RationalGRL development process\footnote{Note that in order to keep the study simple for the users, we did not explicitly ask the respondents to follow the development process from Section~\ref{sect:methodology}}. Users like to be guided during modeling phase, and making arguments explicit in the modeling process was indicated as useful, but argumentation should be integrated into the \emph{process} of goal modeling more than in the goal models themselves, keeping the goal models simple and thus lessening the cognitive overhead. Critical questions seem to be a natural fit for this, as they play a key role in the RationalGRL development process, but in its current form they are only mentioned in the details pane of the tool.



