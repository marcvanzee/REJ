We thank the reviewers for their work.

Reviewer 1 again proposes to accept the paper. Reviewer 2 recommends rejection. The editor proposes that we perform major revisions.

We have worked hard to improve the paper since the first submission. We propose a new framework, inserting argument schemes and formal argument semantics into goal modelling in a way that hasn't been done before. We formally ground this framework in a logical framework for argumentation that has been accepted as a de-facto standard for non-monotonic argumentation inside and outside the field of AI. We give a simple development process, and a fully-working tool. We have further performed an extensive qualitative emprirical coding study (which we call case study in our article), plus a modest expert survey. 

One remark that has been made since the beginning is that, because we propose a new process/language/tool, we should test these in an industrial-strength case-study. However, we remain convinced that a large case study or experiment (where e.g. one compares test and control groups) would take an unreasonable amount of time and space given the material that is alreadty in the paper. See e.g. Estrada et al. (2006) "An empirical evaluation of the i* framework in a model-based software generation environment", who took 9 months to conduct the case study. Furthermore, there are many papers on goal-modeling and related subjects in REJ which perform less empirical evaluation than we do, and the few that do have industrial-strength case studies evaluate a modeling language or tool that was introduced in an earlier paper. We are not directly aware of papers on goal modeling languages doing extensive experimentation with test and control groups. Hence, we believe that asking us to do an industrial strength case-study or test-control group experiment is unreasonable for this paper, more so because this would likely yield enough new insights for a completely new paper. 

The (smaller) remarks made by reviewer 2 since Revision 1 have been helpful, and we try to accomodate as many of them as possible. We would like to note that we have never simply refused suggestions or ignored comments. In some cases, we have replied at length to the reviewer in the response letter and have not added anything to the paper, but we do not think this counts as a refusal to take comments into account. 

Below we respond to reviewer 2's comments on the latest version of the article in detail. 

======= COMMENT #1
First, there are "broad" and "general" hypotheses given, and based on them Null and Alternative Hypotheses. Still, the latter do not contain the level of significance for accepting or rejecting them based on statistical data and their analyses. (It has to be defined before performing an experiment.)
======= REPLY #1
We have added the significance levels below the hypothesis

======= COMMENT #2
H3 (and its specific counterparts) are strange in their formulation, since usability is not difficult, but use is difficult and usability potentially bad. In English, after "neither" there should be a "nor".
The paragraph apparently trying to motivate H3 sounds actually more like motivating user experience (UX) rather than usability, since testing the latter is first of all more about finding usability problems, something never mentioned in this paper. Also the related Null and Alternative Hypotheses are strange with regard to the given motivation. Much as in my previous review, I really wonder about including usability of such a prototype tool here. (As given later in the paper, this tool does not even have an Undo, and the results were completely inconclusive, anyway.)
======= REPLY #2
Our reasons for including the questions and hypothesis on usability (or UX) were not so that we could argue that said UX is particularly good (or bad). We wanted to show that the usability or UX of the tool did, in our opinion, not interfere greatly with what the participants filled in for the other questions. If a tool is very bad (keeps crashing, impossible to use, unclear interface), participants might be more inclined to be very negative about the rest (i.e. the RationalGRL features) as well. In any case, clearly we were not able to explain our ideas clearly enough, so we deleted H3 and only mention the user experience questions in passing.

======= COMMENT #3
For a comparison, actually a user study would have had to be designed, with independent and dependent variables, and participants trying both approaches in two groups (within-subject study design). In such a setting, something should be really measured in addition to subjective questionnaires. Much as indicated by the participants of the reported study (and as known from the literature), a key point of such argumentation systems is the overhead of providing them. There needs to be a return on invest, ideally in a balanced trade-off. I think that this should have been studied, since I am very much in favor of documenting rationale, but in practice developers will only do that if there is a clear benefit in return. The communication of opinions to others was obviously not really included.
======= REPLY #3
As we argue above, we think this is an unreasonable amount of work to ask given what is already in the paper. Also, it now seems as if our (modest) expert survey is worthless, which we think it is not. 


======= COMMENT #4
A theoretical problem of all the results reported here is the use of statistics like mean (here obviously called "average") and standard deviation based on data  on a Likert scale, since this is not permitted according to Measurement Theory. (I am aware, that this is often done, anyway, especially in practice, but the theoretical problem remains, especially for results in a prestigious scientific journal.) Such statistics could and should be done based on measured data, which are usually on an absolute scale.
In addition, has the probability distribution been tested before applying the t-test?
The specific results for Q6 are really strange.
Finally, as indicated above already, the results on usability of the tool are virtually useless.
======= REPLY #4
There is actually quite some discussion about whether Likert scales can be analysed as interval data - in academia more and more researchers (who publish in prestigious statistics journals) are starting to agree that one is allowed to do this. However, we agree that for our data it is perhaps better to err on the side of caution and use a non-parametric test which makes as little assumptions as possible (i.e. the sign test). Interestingly, the results are not much different: for Q2 (prev. Q6) we now cannot reject H0, where previously we could, but in the previous version (with the t-tests) we already indicated that Q2/Q6 has results which are clearly different from the other related questions.

======= COMMENT #5
The first argument on internal validity is strange and really not convincing. If the modeling task given in the experiment was not realistic (and this is most likely the case), then the case study does not help, see below.
I agree that a comparison with a control group is necessary!
-       Discussion
And yes,  a realistic study on this approach needs to involve asking CQs in the course of a development process.
Why did you not let users being guided by your process in the course of a study, when users like that? This would have provided evidence or at least insights on the usefulness of this process, while it is just a claim as it stands.
======= REPLY #5
Again: yes there are many ways in which we could have done our experiments and surveys differently, better, bigger etc. However, the reviewer makes it sound as if the current survey is next to useless, and we strongly disagree with this. It is a modest survey with a fairly high response rate, particularly as all 16 participants have significant expertise in software and requirements engineering - we could do an experiment with 80 inexperienced undergraduates, but what would that really tell us? The answers to the survey questions point in interesting future directions, and overall the experiment gives support to the idea that adding arguments to goal models is the way to go.

======= COMMENT #6
In this context, let me also say a few words on the "Case Study":
-       It cannot be taken as evidence for the approach, since studying recorded design sessions (without goal modeling) is reported in this paper as the basis for creating the argumentation structures in the first place. And this was done by the authors themselves based on their intuition.
-       Actually, this is not really a case study in the sense used in scientific literature. As a matter of fact, it also does not contain lessons learned.
======= REPLY #6
We agree that "case study" is perhaps not the most opportune name. It is, if you want to be precise, a qualitative empirical coding study with a priori coding. We have changed the text in the paper to reflect this.

======= COMMENT #7
In addition, I am still not convinced by the argument for dismissing formal reasoning in propositional logic. After all, it is not theoretically difficult and tools exist. So, what is the point of presenting special-purpose "algorithms" here?
======= REPLY #7
The argumentation formalism we propose is a non-monotonic logic, that is, earlier results might not be true when new information is added. Formal reasoning in propositional logic, taking the semantics of classical logic, is inherently monotonic: you can add propositions (arguments, goals), but these will never change the status (true/false) of existing propositions. So what is needed is a new algorithm that shows what happens if new information is added, we cannot simply take an existing tool (theorem prover) for classical logic.

======= COMMENT #8
Finally, "defending" a binary framework simply by a reference is insufficient. Since the authors propose such a framework, they need to clearly point out its limitations in this regard.
======= REPLY #8
We already included some discussion in the previous revision. We have included an extra sentence on why we think numerical, more fine-grained frameworks are not very suitable. Furthermore, note that the framework is not "binary" in the sense that arguments can only be true or false (as in classical logic): arguments which are at one point considered defeated or overruled can later become justified again if new arguments are added.

======= COMMENT #9
From an editorial perspective, a careful proof reading of grammar would be necessary.
======= REPLY #9
This remark makes it sound as if there are *many* grammatical mistakes in the paper.
